{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to Unsupervised Learning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2kMRCKBKdFd3vKdNwxXuY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NUELBUNDI/Machine-Learning-Projects/blob/main/Introduction_to_Unsupervised_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsupervised machine learning refers to the category of machine learning techniques where models are trained on a dataset without labels. Unsupervised learning is generally use to discover patterns in data and reduce high-dimensional data to fewer dimensions. Here's how unsupervised learning fits into the landscape of machine learning algorithms(source)\n"
      ],
      "metadata": {
        "id": "NuIkgX2ADJQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a full list of unsupervised learning algorithms available in Scikit-learn: https://scikit-learn.org/stable/unsupervised_learning.html"
      ],
      "metadata": {
        "id": "S-GURbhqDm8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering is the process of grouping objects from a dataset such that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (Wikipedia). Scikit-learn offers several clustering algorithms. You can learn more about them here: https://scikit-learn.org/stable/modules/clustering.html"
      ],
      "metadata": {
        "id": "0RvyUdKADw_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some real-world applications of clustering:\n",
        "\n",
        "\n",
        "1. Customer segmentation\n",
        "\n",
        "2. Product recommendation\n",
        "3. Feature engineering\n",
        "\n",
        "4. Anomaly/fraud detection\n",
        "\n",
        "5. Taxonomy creation\n"
      ],
      "metadata": {
        "id": "MDN2_jRmD4Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "k2SOZwaBDLH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris_df=sns.load_dataset('iris')\n"
      ],
      "metadata": {
        "id": "ArXXKRPVEKDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris_df"
      ],
      "metadata": {
        "id": "wV67aq1EERnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=iris_df,x='sepal_length', y='petal_length', hue='species')"
      ],
      "metadata": {
        "id": "kUjYzqPnEUtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = iris_df.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols=iris_df.select_dtypes('object').columns.tolist()"
      ],
      "metadata": {
        "id": "n0RYDjJ2F55g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x= iris_df[numeric_cols]"
      ],
      "metadata": {
        "id": "T6oF8exKHsA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K Means Clustering\n",
        "\n",
        "The K-means algorithm attempts to classify objects into a pre-determined number of clusters by finding optimal central points (called centroids) for each cluster. Each object is classifed as belonging the cluster represented by the closest centroid.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*rw8IUza1dbffBhiA4i0GNQ.png\" width=\"640\">"
      ],
      "metadata": {
        "id": "2ZSVKniVGSwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how the K-means algorithm works:\n",
        "\n",
        "1. Pick K random objects as the initial cluster centers.\n",
        "2. Classify each object into the cluster whose center is closest to the point.\n",
        "3. For each cluster of classified objects, compute the centroid (mean).\n",
        "4. Now reclassify each object using the centroids as cluster centers.\n",
        "5. Calculate the total variance of the clusters (this is the measure of goodness).\n",
        "6. Repeat steps 1 to 6 a few more times and pick the cluster centers with the lowest total variance.\n",
        "\n",
        "Here's a video showing the above steps: https://www.youtube.com/watch?v=4b5d3muPQmA"
      ],
      "metadata": {
        "id": "u4VJIRN9GVpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "fJxG_JEbHX5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=KMeans(n_clusters=3, random_state=42)"
      ],
      "metadata": {
        "id": "AMjdGhrrHdMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x)"
      ],
      "metadata": {
        "id": "X43WiUqmHmaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check the cluster centers for each cluster.\n",
        "\n"
      ],
      "metadata": {
        "id": "rocsewzGH28I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.cluster_centers_"
      ],
      "metadata": {
        "id": "rJSqJO8yH35u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds=model.predict(x)"
      ],
      "metadata": {
        "id": "lOJpzwf9H-9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "id": "Gyx3T0aDIDCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=x, x='sepal_length', y='petal_length', hue=preds);\n",
        "centers_x, centers_y = model.cluster_centers_[:,0], model.cluster_centers_[:,2]\n",
        "plt.plot(centers_x, centers_y, 'xb')"
      ],
      "metadata": {
        "id": "NXOE2rfPIKbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, K-means algorithm was able to classify (for the most part) different specifies of flowers into separate clusters. Note that we did not provide the \"species\" column as an input to `KMeans`."
      ],
      "metadata": {
        "id": "WPaAbYSFIgr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check the \"goodness\" of the fit by looking at `model.inertia_`, which contains the sum of squared distances of samples to their closest cluster center. Lower the inertia, better the fit."
      ],
      "metadata": {
        "id": "icBhUD-xIh1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.inertia_"
      ],
      "metadata": {
        "id": "46i4UaiSIi_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try creating 6 clusters."
      ],
      "metadata": {
        "id": "BaKNlKN3IlEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = KMeans(n_clusters=6, random_state=42).fit(x)\n"
      ],
      "metadata": {
        "id": "POU2gNj1Imgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(X)\n",
        "preds"
      ],
      "metadata": {
        "id": "hlA8-gz4IpFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=x, x='sepal_length', y='petal_length', hue=preds);\n"
      ],
      "metadata": {
        "id": "-nAT5Wl3Ir4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most real-world scenarios, there's no predetermined number of clusters. In such a case, you can create a plot of \"No. of clusters\" vs \"Inertia\" to pick the right number of clusters."
      ],
      "metadata": {
        "id": "7y9OyEn9I4J6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "options = range(2,11)\n",
        "inertias = []\n",
        "\n",
        "for n_clusters in options:\n",
        "    model = KMeans(n_clusters, random_state=42).fit(X)\n",
        "    inertias.append(model.inertia_)\n",
        "    \n",
        "plt.title(\"No. of clusters vs. Inertia\")\n",
        "plt.plot(options, inertias, '-o')\n",
        "plt.xlabel('No. of clusters (K)')\n",
        "plt.ylabel('Inertia');"
      ],
      "metadata": {
        "id": "AXl3buPKI52f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart is creates an \"elbow\" plot, and you can pick the number of clusters beyond which the reduction in inertia decreases sharply."
      ],
      "metadata": {
        "id": "MCyXI3_8JCNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mini Batch K Means**: The K-means algorithm can be quite slow for really large dataset. Mini-batch K-means is an iterative alternative to K-means that works well for large datasets. Learn more about it here: https://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans\n"
      ],
      "metadata": {
        "id": "sImdVxCFJG99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBSCAN\n",
        "\n",
        "Density-based spatial clustering of applications with noise (DBSCAN) uses the density of points in a region to form clusters. It has two main parameters: \"epsilon\" and \"min samples\" using which it classifies each point as a core point, reachable point or noise point (outlier).\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/DBSCAN-Illustration.svg/800px-DBSCAN-Illustration.svg.png\" width=\"400\">\n",
        "\n",
        "Here's a video explaining how the DBSCAN algorithm works: https://www.youtube.com/watch?v=C3r7tGRe2eI"
      ],
      "metadata": {
        "id": "fxzrQ_JuJg4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN"
      ],
      "metadata": {
        "id": "olNB166iJiut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= DBSCAN(eps=1.5, min_samples=5).fit(x)"
      ],
      "metadata": {
        "id": "AZY4XrOPJ1rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=x, x='sepal_length', y='petal_length', hue=model.labels_);\n"
      ],
      "metadata": {
        "id": "bAzma6E6KTm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how the results of DBSCAN and K Means differ:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1339/0*xu3GYMsWu9QiKNOo.png\" width=\"640\">"
      ],
      "metadata": {
        "id": "8RhgD_3AP1Uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hierarchical Clustering\n",
        "\n",
        "Hierarchical clustering, as the name suggests, creates a hierarchy or a tree of clusters.\n",
        "\n",
        "<img src=\"https://dashee87.github.io/images/hierarch.gif\" width=\"640\">\n",
        "\n",
        "While there are several approaches to hierarchical clustering, the most common approach works as follows:\n",
        "\n",
        "1. Mark each point in the dataset as a cluster.\n",
        "2. Pick the two closest cluster centers without a parent and combine them into a new cluster. \n",
        "3. The new cluster is the parent cluster of the two clusters, and its center is the mean of all the points in the cluster.\n",
        "3. Repeat steps 2 and 3 till there's just one cluster left.\n",
        "\n",
        "Watch this video for a visual explanation of hierarchical clustering: https://www.youtube.com/watch?v=7xHsRkOdVwo"
      ],
      "metadata": {
        "id": "dvJGfXtfP3NT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality Reduction and Manifold Learning\n",
        "\n",
        "In machine learning problems, we often encounter datasets with a very large number of dimensions (features or columns). Dimensionality reduction techniques are used to reduce the number of dimensions or features within the data to a manageable or convenient number. \n",
        "\n",
        "\n",
        "Applications of dimensionality reduction:\n",
        "\n",
        "* Reducing size of data without loss of information\n",
        "* Training machine learning models efficiently\n",
        "* Visualizing high-dimensional data in 2/3 dimensions"
      ],
      "metadata": {
        "id": "UlGxcifLP_Om"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principal Component Analysis (PCA)\n",
        "\n",
        "Principal component is a dimensionality reduction technique that uses linear projections of data to reduce their dimensions, while attempting to maximize the variance of data in the projection. Watch this video to learn how PCA works: https://www.youtube.com/watch?v=FgakZw6K1QQ\n",
        "\n",
        "Here's an example of PCA to reduce 2D data to 1D:\n",
        "\n",
        "<img src=\"https://i.imgur.com/ZJ7utlo.png\" width=\"480\">\n",
        "\n",
        "Here's an example of PCA to reduce 3D data to 2D:\n",
        "\n",
        "<img src=\"https://lihan.me/assets/images/pca-illustration.png\" width=\"640\">\n",
        "\n",
        "\n",
        "Let's apply Principal Component Analysis to the Iris dataset."
      ],
      "metadata": {
        "id": "NeUTRumnQBIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "WmwqGq7yQIHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)"
      ],
      "metadata": {
        "id": "KYR5EchcQKvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca.fit(iris_df[numeric_cols])"
      ],
      "metadata": {
        "id": "cJETLsIMQPNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca"
      ],
      "metadata": {
        "id": "0E7e6nY2QQs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed = pca.transform(iris_df[numeric_cols])"
      ],
      "metadata": {
        "id": "yJq2ZDFdQStv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=iris_df['species']);\n"
      ],
      "metadata": {
        "id": "NSThyuAyQTy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the PCA algorithm has done a very good job of separating different species of flowers using just 2 measures."
      ],
      "metadata": {
        "id": "NFlJMEUsQfD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "\n",
        "Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high. Scikit-learn provides many algorithms for manifold learning: https://scikit-learn.org/stable/modules/manifold.html . A commonly-used manifold learning technique is t-Distributed Stochastic Neighbor Embedding or t-SNE, used to visualize high dimensional data in one, two or three dimensions. \n",
        "\n",
        "Here's a visual representation of t-SNE applied to visualize 2 dimensional data in 1 dimension:\n",
        "\n",
        "<img src=\"https://i.imgur.com/rVMAaix.png\" width=\"360\">\n",
        "\n",
        "\n",
        "Here's a visual representation of t-SNE applied to the MNIST dataset, which contains 28px x 28px images of handrwritten digits 0 to 9, a reduction from 784 dimensions to 2 dimensions ([source](https://colah.github.io/posts/2014-10-Visualizing-MNIST/)):\n",
        "\n",
        "<img src=\"https://indico.io/wp-content/uploads/2015/08/mnist-1024x607-1.jpg\" width=\"640\">\n",
        "\n",
        "Here's a video explaning how t-SNE works: https://www.youtube.com/watch?v=NEaUSP4YerM"
      ],
      "metadata": {
        "id": "NAv_Hy6OQgtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE"
      ],
      "metadata": {
        "id": "9ain6dMTQiMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(n_components=2)"
      ],
      "metadata": {
        "id": "5qfvfgr0QkNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed = tsne.fit_transform(iris_df[numeric_cols])"
      ],
      "metadata": {
        "id": "bH3flwRiQl5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=iris_df['species']);\n"
      ],
      "metadata": {
        "id": "Rd5dqLGlQnRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and References\n",
        "\n",
        "<img src=\"https://i.imgur.com/VbVFAsg.png\" width=\"640\">\n",
        "\n",
        "The following topics were covered in this tutorial:\n",
        "\n",
        "- Overview of unsupervised learning algorithms in Scikit-learn\n",
        "- Clustering algorithms: K Means, DBScan, Hierarchical clustering etc.\n",
        "- Dimensionality reduction (PCA) and manifold learning (t-SNE)\n",
        "\n",
        "\n",
        "Check out these resources to learn more:\n",
        "\n",
        "- https://www.coursera.org/learn/machine-learning\n",
        "- https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\n",
        "- https://scikit-learn.org/stable/unsupervised_learning.html\n",
        "- https://scikit-learn.org/stable/modules/clustering.html"
      ],
      "metadata": {
        "id": "NA4QQMG2Q36V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Revision Questions\n",
        "1.\tWhat is unsupervised learning?\n",
        "2.\tWhat is the landscape of Machine Learning algorithms?\n",
        "3.\tWhat is clustering?\n",
        "4.\tWhat are some real-world applications of clustering?\n",
        "5.\tWhat is K-Means clustering?\n",
        "6.\tWhat are centroids?\n",
        "7.\tHow does K-Means algorithm work?\n",
        "8.\tWhat is goodness of fit?\n",
        "9.\tWhat does model.inertia_ contain?\n",
        "10.\tWhat is an elbow plot?\n",
        "11.\tWhat is Mini Batch K-Means?\n",
        "12.\tWhat is DBSCAN?\n",
        "13.\tWhat is Hierarchical clustering? What are it’s common approaches?\n",
        "14.\tWhat is dimensionality reduction?\n",
        "15.\tWhat are the applications of dimensionality reduction?\n",
        "16.\tWhat does PCA stand for? How does it work?\n",
        "17.\tWhat is Manifold learning?\n",
        "18.\tWhat is t-SNE? How does it work?"
      ],
      "metadata": {
        "id": "FfYO8UBERIjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bVR8q_JhROxk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}