{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-Logistic Regression with Python.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPgAVDvqDbwhhrxsK2VT7qa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NUELBUNDI/Machine-Learning-Projects/blob/main/ML_Logistic_Regression_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMy3NygZ2ItF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import plotly.express as px\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read the file\n",
        "\n",
        "url =\"https://raw.githubusercontent.com/NUELBUNDI/Machine-Learning-Projects/main/weatherAUS.csv\"\n",
        "\n",
        "df=pd.read_csv(url)\n",
        "df.shape"
      ],
      "metadata": {
        "id": "UFZEKKCNSu5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the first 5 rows\n",
        "\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "IE-cOZ-0THUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns, end=\",\")"
      ],
      "metadata": {
        "id": "Dp-4fN84TVmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_numerical_data=[]\n",
        "numerical_data=[]\n",
        "for col in df:\n",
        "  results= (df[col].dtype)\n",
        "  if results == 'float64' or results == 'int64':\n",
        "    numerical_data.append(col)\n",
        "  else:\n",
        "    non_numerical_data.append(col)\n",
        "\n",
        "\n",
        "print(f'The Numerical Data  :{numerical_data}\\n')\n",
        "print()\n",
        "print(f'The Non-Numerical Data :{non_numerical_data}\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# numeric_cols = train_inputs.select_dtypes(include=np.number).columns.tolist()\n",
        "# categorical_cols = train_inputs.select_dtypes('object').columns.tolist()\n",
        "  "
      ],
      "metadata": {
        "id": "pHkLUNhPTkgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "id": "tyDwzsYdZEDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop RainTomorrow and RainToday if Nan\n",
        "\n",
        "df.dropna(subset=['RainTomorrow', 'RainToday'], inplace=True)"
      ],
      "metadata": {
        "id": "wMcYQnYWWpDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis and Visualization"
      ],
      "metadata": {
        "id": "DCPHYBUJW_1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "matplotlib.rcParams['font.size']=14\n",
        "matplotlib.rcParams['figure.figsize']=(10,6)\n",
        "matplotlib.rcParams['figure.facecolor'] = '#00000000'"
      ],
      "metadata": {
        "id": "z00C9dcXXE0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "px.histogram(df, x='Location',\n",
        "              title='Location Vs Rainy Day',\n",
        "              color='RainToday')"
      ],
      "metadata": {
        "id": "Qy6aa8adYsFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "px.histogram(df, \n",
        "             x='Temp3pm', \n",
        "             title='Temperature at 3 pm vs. Rain Tomorrow', \n",
        "             color='RainTomorrow')"
      ],
      "metadata": {
        "id": "yyBr76sKZyQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "px.scatter(df.head(2000),\n",
        "           title='Min Temp Vs Max Temp',\n",
        "           x='MinTemp',\n",
        "           y='MaxTemp',\n",
        "           color='RainToday')"
      ],
      "metadata": {
        "id": "nQzQ1QWpZ7Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(16, 12))\n",
        "sns.heatmap(df.corr(),  annot=True ,ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IMpZ7xuCbH7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training, Validation and Test Sets\n"
      ],
      "metadata": {
        "id": "KV-JWeyhcyZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into trin  validate and test\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_val_df , test_df = train_test_split(df, test_size=0.2 , random_state=42)\n",
        "\n",
        "\n",
        "train_df , val_df = train_test_split(train_val_df, test_size=0.25 , random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "print(f'df :{df.shape}')\n",
        "\n",
        "print(f'train_df :{train_df.shape}')\n",
        "\n",
        "print(f'test_df :{test_df.shape}')\n",
        "\n",
        "print(f'val_df :{val_df.shape}')"
      ],
      "metadata": {
        "id": "TyIzMNX99p_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "working with dates, it's often a better idea to separate the training, validation and test sets with time, so that the model is trained on data from the past and evaluated on data from the future.\n",
        "\n",
        "For the current dataset, we can use the Date column in the dataset to create another column for year. We'll pick the last two years for the test set, and one year before it for the validation set."
      ],
      "metadata": {
        "id": "ve5DCjyMBnuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('No of Rows Year')\n",
        "sns.countplot(x=pd.to_datetime(df.Date).dt.year)"
      ],
      "metadata": {
        "id": "bptiJ8noBQuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['year']= pd.to_datetime(df.Date).dt.year\n",
        "\n",
        "df.drop(columns=['year'],inplace=True)"
      ],
      "metadata": {
        "id": "MaOQqim8CY7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['year']= pd.to_datetime(df.Date).dt.year\n",
        "\n",
        "year= pd.to_datetime(df.Date).dt.year\n",
        "\n",
        "train_df=df[year<2015]\n",
        "val_df =df[year==2015]\n",
        "test_df=df[year>2015]\n",
        "\n",
        "print(f'train_df : {train_df.shape}')\n",
        "print(f'val_df : {val_df.shape}')\n",
        "print(f'test_df : {test_df.shape}')"
      ],
      "metadata": {
        "id": "oiRFeZLTB2uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identifying Input and Target Columns\n",
        "\n",
        "\n",
        "Often, not all the columns in a dataset are useful for training a model. In the current dataset, we can ignore the Date column, since we only want to weather conditions to make a prediction about whether it will rain the next day.\n",
        "\n",
        "Let's create a list of input columns, and also identify the target column."
      ],
      "metadata": {
        "id": "C3NTrkUIDUPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_cols= list(train_df.columns)[1:-1]\n",
        "\n",
        "target_col ='RainTomorrow'"
      ],
      "metadata": {
        "id": "DyTwX1uhDYN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_cols)\n",
        "print(target_col)"
      ],
      "metadata": {
        "id": "cykBkaTjDYLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs = train_df[input_cols].copy()\n",
        "train_targets = train_df[target_col].copy()\n",
        "\n",
        "val_inputs = val_df[input_cols].copy()\n",
        "val_targets = val_df[target_col].copy()\n",
        "\n",
        "\n",
        "test_inputs = test_df[input_cols].copy()\n",
        "test_targets = test_df[target_col].copy()"
      ],
      "metadata": {
        "id": "mu-Df1bWD2JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = train_inputs.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = train_inputs.select_dtypes('object').columns.tolist()"
      ],
      "metadata": {
        "id": "WYtwwG5wEUjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs[numeric_cols].describe()"
      ],
      "metadata": {
        "id": "-S8kAPBBENDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the ranges of the numeric columns seem reasonable? If not, we may have to do some data cleaning as well.\n",
        "\n"
      ],
      "metadata": {
        "id": "HhqfFC_wEs09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ChecK nO OF UNIQUE VALUES IN CAATEGORICAL DATA\n",
        "\n",
        "train_inputs[categorical_cols].nunique()"
      ],
      "metadata": {
        "id": "zITGsa1cExU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputing Missing Numeric Data\n",
        "Machine learning models can't work with missing numerical data.\n",
        "\n",
        " The process of filling missing values is called **imputation**."
      ],
      "metadata": {
        "id": "byi3jjmzFel8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check no of missing values\n",
        "\n",
        "df[numeric_cols].isna().sum()\n",
        "\n",
        "train_inputs[numeric_cols].isna().sum()\n",
        "\n",
        "val_inputs[numeric_cols].isna().sum()\n",
        "test_inputs[numeric_cols].isna().sum()"
      ],
      "metadata": {
        "id": "HR48lT_DFPG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several techniques for imputation, but we'll use the most basic one: replacing missing values with the average value in the column using the SimpleImputer class from sklearn.impute"
      ],
      "metadata": {
        "id": "ZkVMkK8UGQd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer"
      ],
      "metadata": {
        "id": "fGw0jg-6GO7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = SimpleImputer(strategy='mean')"
      ],
      "metadata": {
        "id": "ZYkJ1EA1GbRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step in imputation is to fit the imputer to the data i.e. compute the chosen statistic (e.g. mean) for each column in the dataset."
      ],
      "metadata": {
        "id": "EjL8yQjgGnHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputer.fit(df[numeric_cols])"
      ],
      "metadata": {
        "id": "nvDMn7X8Gpls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After calling fit, the computed statistic for each column is stored in the statistics_ property of imputer.\n",
        "\n"
      ],
      "metadata": {
        "id": "K9A6lKpTG6Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(imputer.statistics_)"
      ],
      "metadata": {
        "id": "rVphYhgJG19E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The missing values in the training, test and validation sets can now be filled in using the transform method of imputer."
      ],
      "metadata": {
        "id": "n8mSVnEWHBq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs[numeric_cols]= imputer.transform(train_inputs[numeric_cols])\n",
        "val_inputs[numeric_cols]= imputer.transform(val_inputs[numeric_cols])\n",
        "test_inputs[numeric_cols]= imputer.transform(test_inputs[numeric_cols])"
      ],
      "metadata": {
        "id": "rG7om8OuHCuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The missing values are now filled in with the mean of each column.\n",
        "\n"
      ],
      "metadata": {
        "id": "Cvnd61ZmHdzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs[numeric_cols].isna().sum()"
      ],
      "metadata": {
        "id": "DP_gOzxgHeX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " learn more about other imputation techniques here: https://scikit-learn.org/stable/modules/impute.html"
      ],
      "metadata": {
        "id": "QzlyKM_eIMO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling Numeric Features\n",
        "\n",
        "Another good practice is to scale numeric features to a small range of values e.g. \n",
        "(\n",
        "0\n",
        ",\n",
        "1\n",
        ")\n",
        " or \n",
        "(\n",
        "âˆ’\n",
        "1\n",
        ",\n",
        "1\n",
        ")\n",
        ". Scaling numeric features ensures that no particular feature has a disproportionate impact on the model's loss. Optimization algorithms also work better in practice with smaller numbers.\n",
        "\n",
        "The numeric columns in our dataset have varying ranges."
      ],
      "metadata": {
        "id": "jsaF-92HIXYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use MinMaxScaler from sklearn.preprocessing to scale values to the \n",
        "(\n",
        "0\n",
        ",\n",
        "1\n",
        ")\n",
        " range"
      ],
      "metadata": {
        "id": "nmUNmK6yIcWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "SwXSZoWEINXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "#First we fit the scaler to data  compute the range of values for each numerica column\n",
        "\n",
        "scaler.fit(df[numeric_cols])"
      ],
      "metadata": {
        "id": "H5AxOzwmInnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the Minimum and Maximum vlaues in each column\n",
        "\n",
        "print(f'Minimum: {list(scaler.data_min_)}')\n",
        "\n",
        "print(f'Maximum: {list(scaler.data_max_)}')"
      ],
      "metadata": {
        "id": "WSepMg3cKL6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now separately scale the training, validation and test sets using the transform method of scaler.\n",
        "\n",
        "\n",
        "train_inputs[numeric_cols] = scaler.transform(train_inputs[numeric_cols])\n",
        "val_inputs[numeric_cols] = scaler.transform(val_inputs[numeric_cols])\n",
        "test_inputs[numeric_cols] = scaler.transform(test_inputs[numeric_cols])"
      ],
      "metadata": {
        "id": "Ujq0f2pjNjt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that values in each column lie in range (0,1)\n",
        "\n",
        "train_inputs[numeric_cols].describe()"
      ],
      "metadata": {
        "id": "CbDg0LkvNtNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding Categorical Data\n",
        "\n",
        "Since machine learning models can only be trained with numeric data, we need to convert categorical data to numbers. A common technique is to use one-hot encoding for categorical columns.\n",
        "\n"
      ],
      "metadata": {
        "id": "VT5atdfhN7-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[categorical_cols].nunique()"
      ],
      "metadata": {
        "id": "vwWAUMKOOAn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can perform one hot encoding using the OneHotEncoder class from sklearn.preprocessing.\n",
        "\n"
      ],
      "metadata": {
        "id": "wSHDqu1gOsZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n"
      ],
      "metadata": {
        "id": "LPFWI-m4Oto6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder= OneHotEncoder(sparse=False,handle_unknown='ignore')\n"
      ],
      "metadata": {
        "id": "FDlJacQhOzUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we fit the encoder to the data i.e. identify the full list of categories across all categorical columns.\n",
        "\n"
      ],
      "metadata": {
        "id": "WxghiDFtPEWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.fit(df[categorical_cols])"
      ],
      "metadata": {
        "id": "G_34lntLPFrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.categories_"
      ],
      "metadata": {
        "id": "usIJ652kPRcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The encoder has created a list of categories for each of the categorical columns in the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "4NtAWzEmPcCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can generate column names for each individual category using get_feature_names.\n",
        "\n",
        "encoded_cols= list(encoder.get_feature_names_out(categorical_cols))\n",
        "print(encoded_cols)"
      ],
      "metadata": {
        "id": "ZhCNG-q_Pdpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All of the above columns will be added to train_inputs, val_inputs and test_inputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "FIfN2KxtP7Kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform the encoding, we use the transform method of encoder."
      ],
      "metadata": {
        "id": "QODkm1v6QH8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
        "val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
        "test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])"
      ],
      "metadata": {
        "id": "kcWlrbRRQAxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "7_KSoH5xQcww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs"
      ],
      "metadata": {
        "id": "mwWbnuNTQefK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Processed Data to Disk\n",
        "\n",
        "It can be useful to save processed data to disk, especially for really large datasets, to avoid repeating the preprocessing steps every time you start the Jupyter notebook. The parquet format is a fast and efficient format for saving and loading Pandas dataframes."
      ],
      "metadata": {
        "id": "vjiYr4aPQ29Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('train_inputs:', train_inputs.shape)\n",
        "print('train_targets:', train_targets.shape)\n",
        "print('val_inputs:', val_inputs.shape)\n",
        "print('val_targets:', val_targets.shape)\n",
        "print('test_inputs:', test_inputs.shape)\n",
        "print('test_targets:', test_targets.shape)"
      ],
      "metadata": {
        "id": "3T9fzbL7QnJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs.to_parquet('train_inputs.parquet')\n",
        "val_inputs.to_parquet('val_inputs.parquet')\n",
        "test_inputs.to_parquet('test_inputs.parquet')"
      ],
      "metadata": {
        "id": "XDgxG_tnQv8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "pd.DataFrame(train_targets).to_parquet('train_targets.parquet')\n",
        "pd.DataFrame(val_targets).to_parquet('val_targets.parquet')\n",
        "pd.DataFrame(test_targets).to_parquet('test_targets.parquet')"
      ],
      "metadata": {
        "id": "BdHE-PNWQ0Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can read the data back using pd.read_parquet.\n",
        "\n"
      ],
      "metadata": {
        "id": "yX6OX74QQ_V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "train_inputs = pd.read_parquet('train_inputs.parquet')\n",
        "val_inputs = pd.read_parquet('val_inputs.parquet')\n",
        "test_inputs = pd.read_parquet('test_inputs.parquet')\n",
        "\n",
        "train_targets = pd.read_parquet('train_targets.parquet')[target_col]\n",
        "val_targets = pd.read_parquet('val_targets.parquet')[target_col]\n",
        "test_targets = pd.read_parquet('test_targets.parquet')[target_col]"
      ],
      "metadata": {
        "id": "lVPSSuL_RBMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's verify that the data was loaded properly.\n",
        "\n"
      ],
      "metadata": {
        "id": "4zezc_nIREmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('train_inputs:', train_inputs.shape)\n",
        "print('train_targets:', train_targets.shape)\n",
        "print('val_inputs:', val_inputs.shape)\n",
        "print('val_targets:', val_targets.shape)\n",
        "print('test_inputs:', test_inputs.shape)\n",
        "print('test_targets:', test_targets.shape)"
      ],
      "metadata": {
        "id": "ORXb9wxlRDtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_inputs"
      ],
      "metadata": {
        "id": "0utUAFClRKEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Logistic Regression Model\n",
        "\n",
        "Logistic regression is a commonly used technique for solving binary classification problems. In a logistic regression model:\n",
        "\n",
        "we take linear combination (or weighted sum of the input features)\n",
        "\n",
        "we apply the sigmoid function to the result to obtain a number between 0 and 1\n",
        "\n",
        "this number represents the probability of the input being classified as \"Yes\"\n",
        "\n",
        "instead of RMSE, the cross entropy loss function is used to evaluate the results"
      ],
      "metadata": {
        "id": "VDivK4CFRftq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n"
      ],
      "metadata": {
        "id": "k8OhA1vwRsSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ?LogisticRegression"
      ],
      "metadata": {
        "id": "SGwfZSZ4Sbcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(solver='liblinear')\n"
      ],
      "metadata": {
        "id": "pA61vySdSxHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_inputs[numeric_cols + encoded_cols], train_targets)\n"
      ],
      "metadata": {
        "id": "QGYPh3m8S1Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(numeric_cols + encoded_cols)"
      ],
      "metadata": {
        "id": "oXTSpuUSTZIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.coef_.tolist())"
      ],
      "metadata": {
        "id": "ukPDW4paTbk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.intercept_)"
      ],
      "metadata": {
        "id": "Mr64o4nSTfMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making Predictions and Evaluating the Model\n"
      ],
      "metadata": {
        "id": "HZPPbJG4peuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_inputs[numeric_cols + encoded_cols]\n",
        "X_val = val_inputs[numeric_cols + encoded_cols]\n",
        "X_test = test_inputs[numeric_cols + encoded_cols]"
      ],
      "metadata": {
        "id": "lnnljbl6pmIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_preds= model.predict(X_train)\n",
        "\n",
        "train_preds"
      ],
      "metadata": {
        "id": "pO48QqDdppIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_targets"
      ],
      "metadata": {
        "id": "uGX7fwdJp3aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can output a probabilistic prediction using predict_proba.\n",
        "\n"
      ],
      "metadata": {
        "id": "MK9a1goZp9EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_probs=model.predict_proba(X_train)\n",
        "train_probs"
      ],
      "metadata": {
        "id": "34El7DJqp-LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The numbers above indicate the probabilities for the target classes \"No\" and \"Yes\".\n",
        "\n"
      ],
      "metadata": {
        "id": "8j8WzLmCqNqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can test the accuracy of the model's predictions by computing the percentage of matching values in train_preds and train_targets.\n",
        "This can be done using the accuracy_score function from sklearn.metrics."
      ],
      "metadata": {
        "id": "xThNB5qwqP9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "OmNNZuXCqToD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(train_targets, train_preds)"
      ],
      "metadata": {
        "id": "ZfGPsJUoqhO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model achieves an accuracy of 85.1% on the training set. We can visualize the breakdown of correctly and incorrectly classified inputs using a confusion matrix."
      ],
      "metadata": {
        "id": "JkLKGHyQqqLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "f7wbZm_lqupl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(train_targets, train_preds, normalize='true')"
      ],
      "metadata": {
        "id": "Q9H0wKxlqwhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a helper function to generate predictions, compute the accuracy score and plot a confusion matrix for a given st of inputs."
      ],
      "metadata": {
        "id": "sMqZ48farHuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_plot(inputs, targets, name=''):\n",
        "  preds=model.predict(inputs)\n",
        "\n",
        "  accuracy=accuracy_score(targets, preds)\n",
        "  print(f'Accuracy : {accuracy*100:.2f}')\n",
        "\n",
        "  cf =confusion_matrix(targets,preds,normalize ='true')\n",
        "  plt.figure()\n",
        "  sns.heatmap(cf,annot=True)\n",
        "  plt.xlabel('Prediction')\n",
        "  plt.ylabel('Target')\n",
        "  plt.title((f'{name} Confusion Matrix'))\n",
        "\n",
        "  return preds"
      ],
      "metadata": {
        "id": "p1ROOyMurMK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_preds = predict_and_plot(X_train, train_targets, 'Training')\n"
      ],
      "metadata": {
        "id": "g-eRYTIUsxGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_preds = predict_and_plot(X_val, val_targets, 'Validatiaon')\n"
      ],
      "metadata": {
        "id": "pZSblXRltw3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds = predict_and_plot(X_test, test_targets, 'Test')\n"
      ],
      "metadata": {
        "id": "a4by68zvuB9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy of the model on the test and validation set are above 84%, which suggests that our model generalizes well to data it hasn't seen before.\n",
        "\n",
        "But how good is 84% accuracy? While this depends on the nature of the problem and on business requirements, a good way to verify whether a model has actually learned something useful is to compare its results to a \"random\" or \"dumb\" model.\n",
        "\n",
        "Let's create two models: one that guesses randomly and another that always return \"No\". Both of these models completely ignore the inputs given to them."
      ],
      "metadata": {
        "id": "K2CS8xspuGAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_guess(inputs):\n",
        "    return np.random.choice([\"No\", \"Yes\"], len(inputs))\n",
        "\n",
        "def all_no(inputs):\n",
        "  return np.full(len(inputs), \"No\")\n"
      ],
      "metadata": {
        "id": "KWIh7XamuI3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(test_targets, random_guess(X_test))"
      ],
      "metadata": {
        "id": "Cm3nqi30uQsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(test_targets, all_no(X_test))"
      ],
      "metadata": {
        "id": "2riA8DdpuSAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our random model achieves an accuracy of 50% and our \"always No\" model achieves an accuracy of 77%.\n",
        "\n",
        "Thankfully, our model is better than a \"dumb\" or \"random\" model! This is not always the case, so it's a good practice to benchmark any model you train against such baseline models."
      ],
      "metadata": {
        "id": "9Dm2CFhcuUHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making Predictions on a Single Input\n",
        "\n",
        "Once the model has been trained to a satisfactory accuracy, it can be used to make predictions on new data. Consider the following dictionary containing data collected from the Katherine weather department toda"
      ],
      "metadata": {
        "id": "sHuQXvAUuWNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_input = {'Date': '2021-06-19',\n",
        "             'Location': 'Katherine',\n",
        "             'MinTemp': 23.2,\n",
        "             'MaxTemp': 33.2,\n",
        "             'Rainfall': 10.2,\n",
        "             'Evaporation': 4.2,\n",
        "             'Sunshine': np.nan,\n",
        "             'WindGustDir': 'NNW',\n",
        "             'WindGustSpeed': 52.0,\n",
        "             'WindDir9am': 'NW',\n",
        "             'WindDir3pm': 'NNE',\n",
        "             'WindSpeed9am': 13.0,\n",
        "             'WindSpeed3pm': 20.0,\n",
        "             'Humidity9am': 89.0,\n",
        "             'Humidity3pm': 58.0,\n",
        "             'Pressure9am': 1004.8,\n",
        "             'Pressure3pm': 1001.5,\n",
        "             'Cloud9am': 8.0,\n",
        "             'Cloud3pm': 5.0,\n",
        "             'Temp9am': 25.7,\n",
        "             'Temp3pm': 33.0,\n",
        "             'RainToday': 'Yes'}"
      ],
      "metadata": {
        "id": "LCcpRmtEuZEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to convert the dictionary into a Pandas dataframe, similar to raw_df. This can be done by passing a list containing the given dictionary to the pd.DataFrame constructor."
      ],
      "metadata": {
        "id": "w0QXnuPxuf75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "new_input_df = pd.DataFrame([new_input])"
      ],
      "metadata": {
        "id": "1BaXsc-subiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must now apply the same transformations applied while training the model:\n",
        "\n",
        "Imputation of missing values using the imputer created earlier\n",
        "Scaling numerical features using the scaler created earlier\n",
        "Encoding categorical features using the encoder created earlier"
      ],
      "metadata": {
        "id": "zRk4FgEGujjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_input_df[numeric_cols] = imputer.transform(new_input_df[numeric_cols])\n",
        "new_input_df[numeric_cols] = scaler.transform(new_input_df[numeric_cols])\n",
        "new_input_df[encoded_cols] = encoder.transform(new_input_df[categorical_cols])"
      ],
      "metadata": {
        "id": "GL-_RLSeulZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new_input = new_input_df[numeric_cols + encoded_cols]\n",
        "X_new_input"
      ],
      "metadata": {
        "id": "opuXHEbBuo-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now make a prediction using model.predict."
      ],
      "metadata": {
        "id": "iX8xP-ihureO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(X_new_input)[0]"
      ],
      "metadata": {
        "id": "tr8Va0rlusMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction"
      ],
      "metadata": {
        "id": "IL-2wqMzuvXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob = model.predict_proba(X_new_input)[0]\n",
        "prob"
      ],
      "metadata": {
        "id": "gdp0HATpu3Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a helper function to make predictions for individual inputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "JemkwlWhu8O1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_input(single_input):\n",
        "    input_df = pd.DataFrame([single_input])\n",
        "    input_df[numeric_cols] = imputer.transform(input_df[numeric_cols])\n",
        "    input_df[numeric_cols] = scaler.transform(input_df[numeric_cols])\n",
        "    input_df[encoded_cols] = encoder.transform(input_df[categorical_cols])\n",
        "    X_input = input_df[numeric_cols + encoded_cols]\n",
        "    pred = model.predict(X_input)[0]\n",
        "    prob = model.predict_proba(X_input)[0][list(model.classes_).index(pred)]\n",
        "    return pred, prob"
      ],
      "metadata": {
        "id": "wboK6mdwu9og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_input = {'Date': '2021-06-19',\n",
        "             'Location': 'Launceston',\n",
        "             'MinTemp': 23.2,\n",
        "             'MaxTemp': 33.2,\n",
        "             'Rainfall': 10.2,\n",
        "             'Evaporation': 4.2,\n",
        "             'Sunshine': np.nan,\n",
        "             'WindGustDir': 'NNW',\n",
        "             'WindGustSpeed': 52.0,\n",
        "             'WindDir9am': 'NW',\n",
        "             'WindDir3pm': 'NNE',\n",
        "             'WindSpeed9am': 13.0,\n",
        "             'WindSpeed3pm': 20.0,\n",
        "             'Humidity9am': 89.0,\n",
        "             'Humidity3pm': 58.0,\n",
        "             'Pressure9am': 1004.8,\n",
        "             'Pressure3pm': 1001.5,\n",
        "             'Cloud9am': 8.0,\n",
        "             'Cloud3pm': 5.0,\n",
        "             'Temp9am': 25.7,\n",
        "             'Temp3pm': 33.0,\n",
        "             'RainToday': 'Yes'}"
      ],
      "metadata": {
        "id": "CpYop68-vD9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_input(new_input)"
      ],
      "metadata": {
        "id": "29ZAcFCPvE7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and Loading Trained Models\n",
        "\n",
        "We can save the parameters (weights and biases) of our trained model to disk, so that we needn't retrain the model from scratch each time we wish to use it. Along with the model, it's also important to save imputers, scalers, encoders and even column names. Anything that will be required while generating predictions using the model should be saved.\n",
        "\n",
        "We can use the joblib module to save and load Python objects on the disk."
      ],
      "metadata": {
        "id": "dJN0MVfhvaED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib"
      ],
      "metadata": {
        "id": "JL11-aotvOGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first create a dictionary containing all the required objects.\n",
        "\n"
      ],
      "metadata": {
        "id": "IqKQqbNLveTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rain2morrow = {\n",
        "    'model': model,\n",
        "    'imputer': imputer,\n",
        "    'scaler': scaler,\n",
        "    'encoder': encoder,\n",
        "    'input_cols': input_cols,\n",
        "    'target_col': target_col,\n",
        "    'numeric_cols': numeric_cols,\n",
        "    'categorical_cols': categorical_cols,\n",
        "    'encoded_cols': encoded_cols\n",
        "}"
      ],
      "metadata": {
        "id": "v_GwYX_ivfq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now save this to a file using joblib.dump"
      ],
      "metadata": {
        "id": "6L6tFr3zvuVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(rain2morrow, 'rain2morrow.joblib')"
      ],
      "metadata": {
        "id": "vBWs1fjivohh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rain2morrow = joblib.load('rain2morrow.joblib')"
      ],
      "metadata": {
        "id": "vFeTSjumvwbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the loaded model to make predictions on the original test set."
      ],
      "metadata": {
        "id": "TjKOA1_Uv078"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds2 = rain2morrow['model'].predict(X_test)\n",
        "accuracy_score(test_targets, test_preds2)"
      ],
      "metadata": {
        "id": "0L8FSsmRv2L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting it all Together\n",
        "\n",
        "While we've covered a lot of ground in this tutorial, the number of lines of code for processing the data and training the model is fairly small. Each step requires no more than 3-4 lines of code"
      ],
      "metadata": {
        "id": "c6EG7Z0zwBjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "TwfRKjFKwE9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "\n",
        "# Download the dataset\n",
        "\n",
        "url =\"https://raw.githubusercontent.com/NUELBUNDI/Machine-Learning-Projects/main/weatherAUS.csv\"\n",
        "df=pd.read_csv(url)\n",
        "raw_df=df\n",
        "raw_df.dropna(subset=['RainToday', 'RainTomorrow'], inplace=True)\n",
        "\n",
        "\n",
        "# Create training, validation and test sets\n",
        "year = pd.to_datetime(raw_df.Date).dt.year\n",
        "train_df, val_df, test_df = raw_df[year < 2015], raw_df[year == 2015], raw_df[year > 2015]\n",
        "\n",
        "# Create inputs and targets\n",
        "input_cols = list(train_df.columns)[1:-1]\n",
        "target_col = 'RainTomorrow'\n",
        "train_inputs, train_targets = train_df[input_cols].copy(), train_df[target_col].copy()\n",
        "val_inputs, val_targets = val_df[input_cols].copy(), val_df[target_col].copy()\n",
        "test_inputs, test_targets = test_df[input_cols].copy(), test_df[target_col].copy()\n",
        "\n",
        "# Identify numeric and categorical columns\n",
        "numeric_cols = train_inputs.select_dtypes(include=np.number).columns.tolist()[:-1]\n",
        "categorical_cols = train_inputs.select_dtypes('object').columns.tolist()\n",
        "\n",
        "# Impute missing numerical values\n",
        "imputer = SimpleImputer(strategy = 'mean').fit(raw_df[numeric_cols])\n",
        "train_inputs[numeric_cols] = imputer.transform(train_inputs[numeric_cols])\n",
        "val_inputs[numeric_cols] = imputer.transform(val_inputs[numeric_cols])\n",
        "test_inputs[numeric_cols] = imputer.transform(test_inputs[numeric_cols])\n",
        "\n",
        "# Scale numeric features\n",
        "scaler = MinMaxScaler().fit(raw_df[numeric_cols])\n",
        "train_inputs[numeric_cols] = scaler.transform(train_inputs[numeric_cols])\n",
        "val_inputs[numeric_cols] = scaler.transform(val_inputs[numeric_cols])\n",
        "test_inputs[numeric_cols] = scaler.transform(test_inputs[numeric_cols])\n",
        "\n",
        "# One-hot encode categorical features\n",
        "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(raw_df[categorical_cols])\n",
        "encoded_cols = list(encoder.get_feature_names(categorical_cols))\n",
        "train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
        "val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
        "test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
        "\n",
        "# Save processed data to disk\n",
        "train_inputs.to_parquet('train_inputs.parquet')\n",
        "val_inputs.to_parquet('val_inputs.parquet')\n",
        "test_inputs.to_parquet('test_inputs.parquet')\n",
        "pd.DataFrame(train_targets).to_parquet('train_targets.parquet')\n",
        "pd.DataFrame(val_targets).to_parquet('val_targets.parquet')\n",
        "pd.DataFrame(test_targets).to_parquet('test_targets.parquet')\n",
        "\n",
        "# Load processed data from disk\n",
        "train_inputs = pd.read_parquet('train_inputs.parquet')\n",
        "val_inputs = pd.read_parquet('val_inputs.parquet')\n",
        "test_inputs = pd.read_parquet('test_inputs.parquet')\n",
        "train_targets = pd.read_parquet('train_targets.parquet')[target_col]\n",
        "val_targets = pd.read_parquet('val_targets.parquet')[target_col]\n",
        "test_targets = pd.read_parquet('test_targets.parquet')[target_col]"
      ],
      "metadata": {
        "id": "Tp3izlrcwGex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training and Evaluation"
      ],
      "metadata": {
        "id": "5LKG1ThAwnh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Select the columns to be used for training/prediction\n",
        "X_train = train_inputs[numeric_cols + encoded_cols]\n",
        "X_val = val_inputs[numeric_cols + encoded_cols]\n",
        "X_test = test_inputs[numeric_cols + encoded_cols]\n",
        "\n",
        "# Create and train the model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, train_targets)\n",
        "\n",
        "# Generate predictions and probabilities\n",
        "train_preds = model.predict(X_train)\n",
        "train_probs = model.predict_proba(X_train)\n",
        "accuracy_score(train_targets, train_preds)\n",
        "\n",
        "# Helper function to predict, compute accuracy & plot confustion matrix\n",
        "def predict_and_plot(inputs, targets, name=''):\n",
        "    preds = model.predict(inputs)\n",
        "    accuracy = accuracy_score(targets, preds)\n",
        "    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "    cf = confusion_matrix(targets, preds, normalize='true')\n",
        "    plt.figure()\n",
        "    sns.heatmap(cf, annot=True)\n",
        "    plt.xlabel('Prediction')\n",
        "    plt.ylabel('Target')\n",
        "    plt.title('{} Confusion Matrix'.format(name));    \n",
        "    return preds\n",
        "\n",
        "# Evaluate on validation and test set\n",
        "val_preds = predict_and_plot(X_val, val_targets, 'Validation')\n",
        "test_preds = predict_and_plot(X_test, test_targets, 'Test')\n",
        "\n",
        "# Save the trained model & load it back\n",
        "aussie_rain = {'model': model, 'imputer': imputer, 'scaler': scaler, 'encoder': encoder,\n",
        "               'input_cols': input_cols, 'target_col': target_col, 'numeric_cols': numeric_cols,\n",
        "               'categorical_cols': categorical_cols, 'encoded_cols': encoded_cols}\n",
        "joblib.dump(aussie_rain, 'aussie_rain.joblib')\n",
        "aussie_rain2 = joblib.load('aussie_rain.joblib')"
      ],
      "metadata": {
        "id": "mqL0oEvvwo3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction on Single Inputs"
      ],
      "metadata": {
        "id": "xpbR8hRNwwCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_input(single_input):\n",
        "    input_df = pd.DataFrame([single_input])\n",
        "    input_df[numeric_cols] = imputer.transform(input_df[numeric_cols])\n",
        "    input_df[numeric_cols] = scaler.transform(input_df[numeric_cols])\n",
        "    input_df[encoded_cols] = encoder.transform(input_df[categorical_cols])\n",
        "    X_input = input_df[numeric_cols + encoded_cols]\n",
        "    pred = model.predict(X_input)[0]\n",
        "    prob = model.predict_proba(X_input)[0][list(model.classes_).index(pred)]\n",
        "    return pred, prob\n",
        "\n",
        "new_input = {'Date': '2021-06-19',\n",
        "             'Location': 'Launceston',\n",
        "             'MinTemp': 23.2,\n",
        "             'MaxTemp': 33.2,\n",
        "             'Rainfall': 10.2,\n",
        "             'Evaporation': 4.2,\n",
        "             'Sunshine': np.nan,\n",
        "             'WindGustDir': 'NNW',\n",
        "             'WindGustSpeed': 52.0,\n",
        "             'WindDir9am': 'NW',\n",
        "             'WindDir3pm': 'NNE',\n",
        "             'WindSpeed9am': 13.0,\n",
        "             'WindSpeed3pm': 20.0,\n",
        "             'Humidity9am': 89.0,\n",
        "             'Humidity3pm': 58.0,\n",
        "             'Pressure9am': 1004.8,\n",
        "             'Pressure3pm': 1001.5,\n",
        "             'Cloud9am': 8.0,\n",
        "             'Cloud3pm': 5.0,\n",
        "             'Temp9am': 25.7,\n",
        "             'Temp3pm': 33.0,\n",
        "             'RainToday': 'Yes'}\n",
        "\n",
        "predict_input(new_input)"
      ],
      "metadata": {
        "id": "ZEDd37XmwvY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a logistic regression model, we can use the LogisticRegression class from Scikit-learn. \n",
        "We covered the following topics in this tutorial:\n",
        "\n",
        "Downloading a real-world dataset from Kaggle\n",
        "\n",
        "Exploratory data analysis and visualization\n",
        "\n",
        "Splitting a dataset into training, validation & test sets\n",
        "\n",
        "Filling/imputing missing values in numeric columns\n",
        "\n",
        "Scaling numeric features to a \n",
        "(\n",
        "0\n",
        ",\n",
        "1\n",
        ")\n",
        " range\n",
        "Encoding categorical columns as one-hot vectors\n",
        "\n",
        "Training a logistic regression model using Scikit-learn\n",
        "\n",
        "Evaluating a model using a validation set and test set\n",
        "\n",
        "Saving a model to disk and loading it back"
      ],
      "metadata": {
        "id": "YDeNzf_Uw5Zr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources\n",
        "\n",
        "Check out the following resources to learn more:\n",
        "\n",
        "https://www.youtube.com/watch?v=-la3q9d7AKQ&list=PLNeKWBMsAzboR8vvhnlanxCNr2V7ITuxy&index=1\n",
        "https://www.kaggle.com/prashant111/extensive-analysis-eda-fe-modelling\n",
        "https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction#Baseline\n",
        "https://jovian.ai/aakashns/03-logistic-regression"
      ],
      "metadata": {
        "id": "DRYh0zLYxIAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practise\n",
        "\n",
        "Try training logistic regression models on the following datasets:\n",
        "\n",
        "1. [Breast Cancer detection ](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data)\n",
        "\n",
        "2. [Loan Repayment Prediction](https://www.kaggle.com/competitions/home-credit-default-risk/data)\n",
        "\n"
      ],
      "metadata": {
        "id": "NENI8SJbxPdP"
      }
    }
  ]
}